import asyncio  # ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ìœ„í•œ í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
from playwright.async_api import async_playwright  # Playwright ë¹„ë™ê¸° ë²„ì „
import csv  # CSV íŒŒì¼ ì €ì¥ìš©

# KoBART ìš”ì•½ ëª¨ë¸ì„ ìœ„í•œ Hugging Face ë¼ì´ë¸ŒëŸ¬ë¦¬
from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration

# â–¶ ìš”ì•½ ëª¨ë¸ ë¡œë“œ (KoBART ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì €)
tokenizer = PreTrainedTokenizerFast.from_pretrained("digit82/kobart-summarization")
model = BartForConditionalGeneration.from_pretrained("digit82/kobart-summarization")


# â–¶ ê¸´ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•˜ëŠ” í•¨ìˆ˜ ì •ì˜
def summarize_korean_article(text):
    if len(text) > 1024:
        text = text[:1024]  # ì…ë ¥ ê¸¸ì´ê°€ ëª¨ë¸ í—ˆìš© ë²”ìœ„ ì´ˆê³¼ ì‹œ ìë¥´ê¸°
    inputs = tokenizer(
        [text], max_length=1024, return_tensors="pt", truncation=True
    )  # í† í°í™”
    summary_ids = model.generate(  # ìš”ì•½ ìƒì„±
        inputs["input_ids"],
        max_length=128,
        min_length=10,
        length_penalty=2.0,
        num_beams=4,
        early_stopping=True,
    )
    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)  # í† í° â†’ ë¬¸ì¥


# â–¶ í¬ë¡¤ë§ ëŒ€ìƒ í‚¤ì›Œë“œ ì •ì˜
TARGET_KEYWORDS = [
    "ì‚¼ì„±ì „ì",
    "ì¹´ì¹´ì˜¤",
    "ë„¤ì´ë²„",
    "SKT",
    "í•˜ë½",
    "ìƒìŠ¹",
    "í•œì€",
    "ì‚¼ì„±",
    "LG",
]

# â–¶ CSV ì €ì¥ íŒŒì¼ëª… ë° ë³‘ë ¬ ì²˜ë¦¬ ì œí•œ ìˆ˜
CSV_FILE = "naver_economy_with_summary.csv"
MAX_CONCURRENT = 5  # ë™ì‹œì— ì—´ ìˆ˜ ìˆëŠ” ê¸°ì‚¬ ìˆ˜ ì œí•œ


# â–¶ ê° ë‰´ìŠ¤ ê¸°ì‚¬ì— ëŒ€í•´ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ (ë¹„ë™ê¸°)
async def process_article(context, link, writer, sem):
    async with sem:  # ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œì— MAX_CONCURRENTê°œë§Œ ì‹¤í–‰ë˜ë„ë¡ ì œì–´
        page = await context.new_page()  # ìƒˆ ë¸Œë¼ìš°ì € íƒ­ ì—´ê¸°
        try:
            await page.goto(link, timeout=10000)  # ê¸°ì‚¬ í˜ì´ì§€ ì ‘ì† (ìµœëŒ€ 10ì´ˆ ëŒ€ê¸°)
            await page.wait_for_selector(
                "#dic_area, #newsct_article", timeout=5000
            )  # ë³¸ë¬¸ ë¡œë”© ëŒ€ê¸°

            title = await page.title()  # ê¸°ì‚¬ ì œëª© ì¶”ì¶œ

            # â–¶ ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (PC/Mobile êµ¬ì¡° ëª¨ë‘ ëŒ€ì‘)
            try:
                content = await page.inner_text("#dic_area")
            except:
                try:
                    content = await page.inner_text("#newsct_article")
                except:
                    content = ""  # ë³¸ë¬¸ì´ ì•„ì˜ˆ ì—†ì„ ê²½ìš° ë¹ˆ ë¬¸ìì—´

            # â–¶ ë‚ ì§œ ì •ë³´ ì¶”ì¶œ
            try:
                date = await page.inner_text("span.t11")  # PC êµ¬ì¡°
            except:
                try:
                    date = await page.inner_text(
                        "span.media_end_head_info_datestamp_time"
                    )  # ëª¨ë°”ì¼ êµ¬ì¡°
                except:
                    date = "ë‚ ì§œ ì—†ìŒ"

            # â–¶ ì œëª©ì´ë‚˜ ë³¸ë¬¸ì— í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ í™•ì¸
            if any(keyword in (title + content) for keyword in TARGET_KEYWORDS):
                summary = summarize_korean_article(content)  # ìš”ì•½ ìƒì„±
                writer.writerow(
                    [title.strip(), link, date.strip(), summary.strip()]
                )  # CSV ì €ì¥
                print(f"âœ… ì €ì¥ ì™„ë£Œ: {title}")
            else:
                print(f"â í‚¤ì›Œë“œ ë¯¸í¬í•¨ â†’ {title}")

        except Exception as e:
            print(f"âš ï¸ ì—ëŸ¬ ë°œìƒ: {e} / ë§í¬: {link}")
        finally:
            await page.close()  # ë¸Œë¼ìš°ì € íƒ­ ë‹«ê¸°


# â–¶ ì „ì²´ ë‰´ìŠ¤ í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•˜ê³  ê° ê¸°ì‚¬ ì²˜ë¦¬í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜
async def scrape_naver_news():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)  # ë¸Œë¼ìš°ì € ì‹¤í–‰ (í™”ë©´ ì—†ì´)
        context = await browser.new_context()
        page = await context.new_page()

        # â–¶ ë„¤ì´ë²„ ê²½ì œ ë‰´ìŠ¤ ëª©ë¡ í˜ì´ì§€ ì ‘ì†
        await page.goto(
            "https://news.naver.com/main/list.naver?mode=LSD&mid=sec&sid1=101"
        )

        # â–¶ ê¸°ì‚¬ ë§í¬ë“¤ ì¶”ì¶œ (ì´ë¯¸ì§€ ì—†ëŠ” ì œëª© ë§í¬ë§Œ)
        article_links = await page.eval_on_selector_all(
            "ul.type06_headline li > dl > dt:not(.photo) > a",
            "elements => elements.map(el => el.href)",
        )

        print(f"ğŸ”— ìˆ˜ì§‘ëœ ê¸°ì‚¬ ë§í¬ ìˆ˜: {len(article_links)}")

        # â–¶ ì¤‘ë³µ ë§í¬ ì œê±°
        seen = set()
        # filtered_links = [
        #     link for link in article_links if link not in seen and not seen.add(link)
        # ]
        filtered_links = []
        for link in article_links:
            if link not in seen:
                seen.add(link)
                filtered_links.append(link)

        # â–¶ ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œì— ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ê¸°ì‚¬ ìˆ˜ ì œí•œ
        sem = asyncio.Semaphore(MAX_CONCURRENT)

        # â–¶ CSV íŒŒì¼ ì—´ê³  ì €ì¥ ì‹œì‘
        with open(CSV_FILE, "w", newline="", encoding="utf-8-sig") as f:
            writer = csv.writer(f)
            writer.writerow(["ì œëª©", "ë§í¬", "ë‚ ì§œ", "ìš”ì•½"])  # í—¤ë”

            # â–¶ ê° ê¸°ì‚¬ ë§í¬ë§ˆë‹¤ ì²˜ë¦¬ íƒœìŠ¤í¬ ìƒì„± â†’ ë™ì‹œì— ì‹¤í–‰
            tasks = [
                process_article(context, link, writer, sem) for link in filtered_links
            ]
            await asyncio.gather(*tasks)  # ëª¨ë“  ê¸°ì‚¬ ë³‘ë ¬ ì²˜ë¦¬

        await browser.close()  # ë¸Œë¼ìš°ì € ì¢…ë£Œ
        print("ğŸ‰ í¬ë¡¤ë§ ë° ìš”ì•½ ì™„ë£Œ!")


# â–¶ í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì§„ì…ì 
asyncio.run(scrape_naver_news())
